{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import time\n",
    "import random\n",
    "\n",
    "from yahoofinancials import YahooFinancials\n",
    "import math\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.neighbors import LocalOutlierFactor as LOF\n",
    "from sklearn.svm import OneClassSVM\n",
    "from scipy.stats import entropy\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import energy_distance\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import warnings\n",
    "from scipy.special import kl_div\n",
    "import csv\n",
    "from datetime import date, timedelta\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical metrics processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a function to parse JSON files from https://financialmodelingprep.com/\n",
    "2. Base on Apple ticket, retrieve the list of key statistics. The only one excluded - date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Revenue per Share',\n",
       " 'Net Income per Share',\n",
       " 'Operating Cash Flow per Share',\n",
       " 'Free Cash Flow per Share',\n",
       " 'Cash per Share',\n",
       " 'Book Value per Share',\n",
       " 'Tangible Book Value per Share',\n",
       " 'Shareholders Equity per Share',\n",
       " 'Interest Debt per Share',\n",
       " 'Market Cap',\n",
       " 'Enterprise Value',\n",
       " 'PE ratio',\n",
       " 'Price to Sales Ratio',\n",
       " 'POCF ratio',\n",
       " 'PFCF ratio',\n",
       " 'PB ratio',\n",
       " 'PTB ratio',\n",
       " 'EV to Sales',\n",
       " 'Enterprise Value over EBITDA',\n",
       " 'EV to Operating cash flow',\n",
       " 'EV to Free cash flow',\n",
       " 'Earnings Yield',\n",
       " 'Free Cash Flow Yield',\n",
       " 'Debt to Equity',\n",
       " 'Debt to Assets',\n",
       " 'Net Debt to EBITDA',\n",
       " 'Current ratio',\n",
       " 'Interest Coverage',\n",
       " 'Income Quality',\n",
       " 'Dividend Yield',\n",
       " 'Payout Ratio',\n",
       " 'SG&A to Revenue',\n",
       " 'R&D to Revenue',\n",
       " 'Intangibles to Total Assets',\n",
       " 'Capex to Operating Cash Flow',\n",
       " 'Capex to Revenue',\n",
       " 'Capex to Depreciation',\n",
       " 'Stock-based compensation to Revenue',\n",
       " 'Graham Number',\n",
       " 'Graham Net-Net',\n",
       " 'Working Capital',\n",
       " 'Tangible Asset Value',\n",
       " 'Net Current Asset Value',\n",
       " 'Invested Capital',\n",
       " 'Average Receivables',\n",
       " 'Average Payables',\n",
       " 'Average Inventory',\n",
       " 'Capex per Share']"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_jsonparsed_data(url):\n",
    "    response = urlopen(url)\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "url = (\"https://financialmodelingprep.com/api/v3/company-key-metrics/AAPL?period=quarter\")\n",
    "apple = get_jsonparsed_data(url)\n",
    "\n",
    "keys = []\n",
    "for key in apple['metrics'][0]:\n",
    "    if not key == 'date':\n",
    "        keys.append(key)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_symbols = ['AAPL', 'ABT', 'ABBV', 'ACN', 'ACE', 'ADBE', 'ADT', 'AAP', 'AES', 'AET', 'AFL', 'AMG', 'A', 'GAS', 'ARE', 'APD', 'AKAM', 'AA', 'AGN', 'ALXN', 'ALLE', 'ADS', 'ALL', 'ALTR', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'APC', 'ADI', 'AON', 'APA', 'AIV', 'AMAT', 'ADM', 'AIZ', 'T', 'ADSK', 'ADP', 'AN', 'AZO', 'AVGO', 'AVB', 'AVY', 'BHI', 'BLL', 'BAC', 'BK', 'BCR', 'BXLT', 'BAX', 'BBT', 'BDX', 'BBBY', 'BRK.B', 'BBY', 'BLX', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'BRCM', 'BF.B', 'CHRW', 'CA', 'CVC', 'COG', 'CAM', 'CPB', 'COF', 'CAH', 'HSIC', 'KMX', 'CCL', 'CAT', 'CBG', 'CBS', 'CELG', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHK', 'CVX', 'CMG', 'CB', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CTXS', 'CLX', 'CME', 'CMS', 'COH', 'KO', 'CCE', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CSC', 'CAG', 'COP', 'CNX', 'ED', 'STZ', 'GLW', 'COST', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DLPH', 'DAL', 'XRAY', 'DVN', 'DO', 'DTV', 'DFS', 'DISCA', 'DISCK', 'DG', 'DLTR', 'D', 'DOV', 'DOW', 'DPS', 'DTE', 'DD', 'DUK', 'DNB', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMC', 'EMR', 'ENDP', 'ESV', 'ETR', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'EXC', 'EXPE', 'EXPD', 'ESRX', 'XOM', 'FFIV', 'FB', 'FAST', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC', 'FTI', 'F', 'FOSL', 'BEN', 'FCX', 'FTR', 'GME', 'GPS', 'GRMN', 'GD', 'GE', 'GGP', 'GIS', 'GM', 'GPC', 'GNW', 'GILD', 'GS', 'GT', 'GOOGL', 'GOOG', 'GWW', 'HAL', 'HBI', 'HOG', 'HAR', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HCN', 'HP', 'HES', 'HPQ', 'HD', 'HON', 'HRL', 'HSP', 'HST', 'HCBK', 'HUM', 'HBAN', 'ITW', 'IR', 'INTC', 'ICE', 'IBM', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IRM', 'JEC', 'JBHT', 'JNJ', 'JCI', 'JOY', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'GMCR', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KRFT', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LM', 'LEG', 'LEN', 'LVLT', 'LUK', 'LLY', 'LNC', 'LLTC', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MNK', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MJN', 'MMV', 'MDT', 'MRK', 'MET', 'KORS', 'MCHP', 'MU', 'MSFT', 'MHK', 'TAP', 'MDLZ', 'MON', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MUR', 'MYL', 'NDAQ', 'NOV', 'NAVI', 'NTAP', 'NFLX', 'NWL', 'NFX', 'NEM', 'NWSA', 'NEE', 'NLSN', 'NKE', 'NI', 'NE', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'OI', 'PCAR', 'PLL', 'PH', 'PDCO', 'PAYX', 'PNR', 'PBCT', 'POM', 'PEP', 'PKI', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PBI', 'PCL', 'PNC', 'RL', 'PPG', 'PPL', 'PX', 'PCP', 'PCLN', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RRC', 'RTN', 'O', 'RHT', 'REGN', 'RF', 'RSG', 'RAI', 'RHI', 'ROK', 'COL', 'ROP', 'ROST', 'RLD', 'R', 'CRM', 'SNDK', 'SCG', 'SLB', 'SNI', 'STX', 'SEE', 'SRE', 'SHW', 'SPG', 'SWKS', 'SLG', 'SJM', 'SNA', 'SO', 'LUV', 'SWN', 'SE', 'STJ', 'SWK', 'SPLS', 'SBUX', 'HOT', 'STT', 'SRCL', 'SYK', 'STI', 'SYMC', 'SYY', 'TROW', 'TGT', 'TEL', 'TE', 'TGNA', 'THC', 'TDC', 'TSO', 'TXN', 'TXT', 'HSY', 'TRV', 'TMO', 'TIF', 'TWX', 'TWC', 'TJX', 'TMK', 'TSS', 'TSCO', 'RIG', 'TRIP', 'FOXA', 'TSN', 'TYC', 'UA', 'UNP', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'URBN', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'DIS', 'WM', 'WAT', 'ANTM', 'WFC', 'WDC', 'WU', 'WY', 'WHR', 'WFM', 'WMB', 'WEC', 'WYN', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XL', 'XYL', 'YHOO', 'YUM', 'ZBH', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the statistical data for all SP500 companies \n",
    "data = []\n",
    "for symbol in SP500_symbols:\n",
    "    url = \"https://financialmodelingprep.com/api/v3/company-key-metrics/{}?period=quarter\".format(symbol)\n",
    "    data.append(get_jsonparsed_data(url))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only those companies which has metrics\n",
    "cleaned_data = []\n",
    "for symbol in data:\n",
    "    try:\n",
    "        symbol['metrics']\n",
    "        cleaned_data.append(symbol)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only those companies which has all 41 metric\n",
    "full_info_data = []\n",
    "for symbol in cleaned_data:\n",
    "    try:\n",
    "        if len(symbol['metrics']) == 41:\n",
    "            full_info_data.append(symbol)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert date from dictionary key-value representation, to the matrix of features\n",
    "resulted `database` variable is 3-D. \n",
    "1. First dimention - the list of periods (quarterly)\n",
    "2. Second dimention - the list of companies\n",
    "3. Third - list of features described given company in a given period (taken from key metrics)\n",
    "\n",
    "So, for example [0][2] - is an array of features for the company with index 2 in the first quartal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "databases = []\n",
    "for i in range(41):\n",
    "    companies = []\n",
    "    for symbol in full_info_data:\n",
    "        company = []\n",
    "        for key in keys:\n",
    "            try:\n",
    "                company.append(float(symbol['metrics'][i][key]))\n",
    "            except:\n",
    "                company.append(0)\n",
    "        companies.append(company)\n",
    "    databases.append(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each quartal we need to define the kernal = distance matrix between companies. \n",
    "# For that firstly normalize the data, and then calculate rbf kernel\n",
    "def kernalise_database(database, norm=\"max\", gamma=1)\n",
    "    kernalizes_databases = []\n",
    "    for base in databases:\n",
    "        normal = normalize(np.array(base), norm=\"max\")\n",
    "        kernalizes_databases.append(rbf_kernel(normal, normal, gamma=1))\n",
    "    return kernalizes_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "390\n"
     ]
    }
   ],
   "source": [
    "# the list of all symbols which are in the database (~has full set of statistics in a given period of time)\n",
    "keys_list = []\n",
    "for symbol in full_info_data:\n",
    "    keys_list.append(symbol['symbol'])\n",
    "print(len(keys_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prices processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_data(point):\n",
    "    return point['adjclose']\n",
    "\n",
    "def create_distance_matrix(historical_prices, approach='corr'):\n",
    "    prices_matrix = []\n",
    "#   need to normalize data by the percantage of change -> escape effect of different scale\n",
    "    for stock_prices in historical_prices:\n",
    "        new_array = stock_prices.copy() # single stock time serie\n",
    "        for i, element in enumerate(stock_prices):\n",
    "            if i == 0 or stock_prices[i-1] == 0:\n",
    "                new_array[i] = 10E-20 #initial change is zero\n",
    "            else:\n",
    "                new_array[i] = (stock_prices[i] - stock_prices[i-1])/stock_prices[i-1]\n",
    "            \n",
    "            if math.isnan(new_array[i]) or new_array[i] == 0:\n",
    "                new_array[i] = 10E-20\n",
    "#         build matrix back\n",
    "        if len(prices_matrix) > 0:\n",
    "            prices_matrix = np.vstack((prices_matrix, new_array))\n",
    "        else:\n",
    "            prices_matrix = np.array(new_array)\n",
    "            \n",
    "#     now prices_matrix - contains list of price changes for each company in a given period\n",
    "# the next step - create a kernel (distance matrix) For that there are three possible approaches\n",
    "\n",
    "# defaul one - correlation\n",
    "    if approach=='corr':\n",
    "        return np.absolute(np.corrcoef(prices_matrix))\n",
    "    \n",
    "#     entropy between two series\n",
    "    if approach=='KL':\n",
    "        distances_matrix = np.zeros((len(prices_matrix), len(prices_matrix)))\n",
    "        for i, stock in enumerate(prices_matrix):\n",
    "            for j, stock_2 in enumerate(prices_matrix):\n",
    "                distances_matrix[i][j] = entropy(stock, qk=stock_2)\n",
    "        return distances_matrix\n",
    "    \n",
    "#     custom - two companies closer when they have more corelated changed (the same direction of change in a day)\n",
    "    if approach=='custom':\n",
    "        distances_matrix = np.zeros((len(prices_matrix), len(prices_matrix)))\n",
    "        np.shape(distance_matrix)\n",
    "        for i, stock in enumerate(prices_matrix):\n",
    "            for j, stock_2 in enumerate(prices_matrix):\n",
    "                k = 0\n",
    "                for q, price in enumerate(stock):\n",
    "                    if (price > 0 and stock_2[q] > 0) or (price <= 0 and stock_2[q] <= 0):\n",
    "                        k += 1\n",
    "                if abs(k/len(stock) - 0.5) * 2 > 1:\n",
    "                    print(k, abs(k/len(stock) - 0.5) * 2)\n",
    "                distances_matrix[i][j] = abs(k/len(stock) - 0.5) * 2\n",
    "        return distances_matrix\n",
    "\n",
    "# from YahooFinancial dataset to price matrix\n",
    "def create_prices_matrix(historical_stock_prices):\n",
    "    prices_matrix = []\n",
    "    company_names = []\n",
    "    for stock_name in historical_stock_prices:\n",
    "        try:\n",
    "            new_array = list(map(convert_data, historical_stock_prices[stock_name]['prices']))\n",
    "            if len(prices_matrix) > 0:\n",
    "                prices_matrix = np.vstack((prices_matrix, new_array))\n",
    "            else:\n",
    "                prices_matrix = np.array(new_array)\n",
    "            company_names.append(stock_name)\n",
    "        except:\n",
    "            pass\n",
    "    return prices_matrix, company_names   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find indexes of stat-dataset which doesn't have corresponding proper prices-set - this indexes will be removed\n",
    "# in order to make stat-matrixes the same dimensionality with price-matrixes\n",
    "def find_extra_keys(names):\n",
    "    extra_symbols = []\n",
    "    for i, key in enumerate(keys_list):\n",
    "        if not key in names:\n",
    "            extra_symbols.append(i)\n",
    "    return extra_symbols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns two 3d matrix. The first one - prices, second - statistical\n",
    "def load_prices_information():\n",
    "    # original price matrixes (without normalisation and calculating the distance)\n",
    "    price_original_matrixes = []\n",
    "    # list of the companie's name which has full price history in a given quartal\n",
    "    price_matrix_names = []\n",
    "    # statistical matrixes without companies which doesn't have complete prices \n",
    "    adjusted_statistical_matrix = []\n",
    "    # number of calendar days in each quartal\n",
    "    intervals = [91, 91, 89, 90]\n",
    "    # start date, and then with each step move it on corresponding number of days\n",
    "    current_date = date(2009, 7, 1)\n",
    "    # number of quartals\n",
    "    number_of_intervals = len(kernalizes_databases)\n",
    "    for i in range(number_of_intervals):\n",
    "    #   calculate start and end date for given № quartal\n",
    "        start_date = \"\"\n",
    "        end_date = \"\"\n",
    "        increased_days = 0\n",
    "    #   for leap year, there is one additional day in the first quartal\n",
    "        if i == 10 or i == 26:\n",
    "            increased_days = 1\n",
    "        start_date = current_date.strftime(\"%Y-%m-%d\")\n",
    "        end_date = (current_date + timedelta(days=intervals[i%4] + increased_days)).strftime(\"%Y-%m-%d\")\n",
    "        current_date = current_date + timedelta(days=intervals[i%4] + 1 + increased_days)\n",
    "\n",
    "    #   get daily historical prices for companies which has statistical data\n",
    "        historical_price_data = YahooFinancials(keys_list).get_historical_price_data(start_date, end_date, 'daily')\n",
    "        matrix, names = create_prices_matrix(historical_price_data)\n",
    "\n",
    "        price_original_matrixes.append(matrix)\n",
    "        price_matrix_names.append(names)\n",
    "\n",
    "    #   execution of this block is very long (more than an hour) - this is the track of progress\n",
    "        print(i, start_date, end_date)\n",
    "        \n",
    "    for i in range(1, number_of_intervals):\n",
    "        # some companies doesn't have proper prices list - remove them from statistical databes in corresponding quartal\n",
    "        extra_keys = find_extra_keys(price_matrix_names[i])\n",
    "        # minus one interval because stats and prices never comes together in one quartal. All statistics comes with delay\n",
    "        # from 1 month to a quartal. Therefore all future combination of stats and prices will be with 1-quartal delay\n",
    "        # for example 1th quartal 2010 of statistical data with 2nd quartal 2010 of prices\n",
    "        stat_mat = np.delete(np.delete(kernalizes_databases[i-1], extra_keys, 0), extra_keys, 1)\n",
    "        adjusted_statistical_matrix.append(stat_mat)\n",
    "#     append the last stat matrix without modification - it will not be used, just to keep dimensionality the same\n",
    "    adjusted_statistical_matrix.append(kernalizes_databases[-1])\n",
    "    return price_original_matrixes, adjusted_statistical_matrix\n",
    "    \n",
    "    \n",
    "# form distance matrixes from prices. Approach = 'corr'/'KL'/'custom'\n",
    "# original matrixes - 3D quartal-company-prices\n",
    "def calculate_price_distances(original_matrixes, approach='corr'):\n",
    "        # matrixes of distances between companies based on price and default approach (correlation)\n",
    "    price_distance_matrixes = []\n",
    "    for matrix in original_matrixes:\n",
    "        distance_matrix = create_distance_matrix(historical_prices=matrix, approach=approach)\n",
    "        price_distance_matrixes.append(distance_matrix)\n",
    "    return price_distance_matrixes\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix: 2D array of distance measure of variables\n",
    "# trevial approach based on mean distance \n",
    "# return the list of indexes with detected anomalies\n",
    "def find_anomaly_custom(matrix, per_out):\n",
    "    result = []\n",
    "    mean_distance = np.mean(matrix) / 3\n",
    "    # if cloud_number of nearest points located father than mean_distance, then it's anomaly  \n",
    "    cloud_number = 10\n",
    "    avg_distances = []\n",
    "    for i, stock in enumerate(matrix):\n",
    "        A = np.array(stock)\n",
    "        idx = np.argpartition(A, cloud_number)\n",
    "        avg_distances.append(np.mean(A[idx[:cloud_number]]))\n",
    "        \n",
    "    number_anomaly = int(per_out * len(matrix[0]))\n",
    "    return np.argpartition(np.array(avg_distances), number_anomaly)[:number_anomaly]\n",
    "\n",
    "# base on local outlier factor\n",
    "def find_anomaly_lof(matrix, per_out):\n",
    "    detector = LOF(metric='precomputed', contamination=per_out)\n",
    "    inlines = detector.fit_predict(matrix)\n",
    "    result = []\n",
    "    for i, res in enumerate(inlines):\n",
    "        if res == -1:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "# based on SVM\n",
    "def find_anomaly_svm(matrix, per_out):\n",
    "    detector = OneClassSVM(kernel='precomputed', nu=per_out)\n",
    "    inlines = detector.fit_predict(matrix)\n",
    "    result = []\n",
    "    for i, res in enumerate(inlines):\n",
    "        if res == -1:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "# use pre-computed matrix of distances!!\n",
    "# general function to find outliers, approach one of ['custom', 'lof', 'svm']\n",
    "def find_anomaly(matrix, approach, per_out):\n",
    "    if approach=='custom':\n",
    "        return find_anomaly_custom(matrix, per_out)\n",
    "    if approach=='lof':\n",
    "        return find_anomaly_lof(matrix, per_out)\n",
    "    if approach=='svm':\n",
    "        return find_anomaly_svm(matrix, per_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two distance(kernel) matrixes. Statistical and price. \n",
    "\n",
    "stat_1, price_1 - training period.\n",
    "stat_2, price_2 - test period\n",
    "\n",
    "1. Apply sum or multiply operator for kernels. \n",
    "2. Based on new kernel, apply anomaly detector approach.\n",
    "3. Compare two arrays of anomalieys. \n",
    "4. Return two values: the proporation of anomalies from the first (training) set which are also exist in the second (test) set. And the number of detected anomalies in the first set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(stat_1, price_1, stat_2, price_2, detector, per_out, operation='sum'):\n",
    "    K = (stat_1 + price_1) / 2\n",
    "    K_test = (stat_2 + price_2) / 2\n",
    "    if operation == 'mult':\n",
    "        K = stat_1 * price_1\n",
    "        K_test = stat_2 * price_2\n",
    "    train_anomaly = find_anomaly(K, detector, per_out)\n",
    "    test_anomaly = find_anomaly(K_test, detector, per_out)\n",
    "    predicted_anomalies = 0\n",
    "    correct_anomalies = []\n",
    "    for anomaly in train_anomaly:\n",
    "        if anomaly in test_anomaly:\n",
    "            correct_anomalies.append(keys_list[anomaly])\n",
    "            predicted_anomalies += 1\n",
    "    return predicted_anomalies/len(train_anomaly), len(train_anomaly), correct_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the information about copmanies (name/sector/industry)\n",
    "# the first argument - the postfix for filename\n",
    "# the second - list of tickets of companies \n",
    "def print_table(number, companies):\n",
    "#   get keys\n",
    "    url = \"https://financialmodelingprep.com/api/v3/company/profile/AAPL\"\n",
    "    default_data = get_jsonparsed_data(url)['profile']\n",
    "    with open('companies_{}.csv'.format(number), 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile, delimiter='!',\n",
    "                            quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "        writer.writerow(default_data.keys())\n",
    "        for symbol in companies:\n",
    "            url = \"https://financialmodelingprep.com/api/v3/company/profile/{}\".format(symbol)\n",
    "            data = get_jsonparsed_data(url)['profile']\n",
    "            writer.writerow([data[x] for x in default_data.keys()])\n",
    "#                 [data['companyName'], data['sector'], data['industry']])\n",
    "\n",
    "# print_table(8, ['AAP', 'ALXN', 'AMZN', 'AAL', 'ABC', 'ADP', 'BLL', 'BAC', 'BBBY', 'CNP', 'CTXS', 'DE', 'DISCA', 'EMR', 'FITB', 'F', 'FCX', 'GPC', 'GILD', 'IP', 'MDLZ', 'MCO', 'NTAP', 'NWL', 'NSC', 'PFE', 'O', 'SJM', 'SWN', 'TEL', 'VRSN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-334-5de284aa4b42>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprice_original_matrixes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madjusted_statistical_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_prices_information\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-315-cb68095b7b94>\u001b[0m in \u001b[0;36mload_prices_information\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#   get daily historical prices for companies which has statistical data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mhistorical_price_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mYahooFinancials\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeys_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_historical_price_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'daily'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mmatrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_prices_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistorical_price_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/yahoofinancials/__init__.py\u001b[0m in \u001b[0;36mget_historical_price_data\u001b[0;34m(self, start_date, end_date, time_interval)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_date\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0mhist_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'start'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'interval'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minterval_code\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_stock_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'history'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_obj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhist_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;31m# Private Method for Functions needing stock_price_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/yahoofinancials/__init__.py\u001b[0m in \u001b[0;36mget_stock_data\u001b[0;34m(self, statement_type, tech_type, report_name, hist_obj)\u001b[0m\n\u001b[1;32m    476\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtick\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mticker\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 478\u001b[0;31m                     \u001b[0mdict_ent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_dict_ent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtick\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatement_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtech_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreport_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    479\u001b[0m                     \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdict_ent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mManagedException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/yahoofinancials/__init__.py\u001b[0m in \u001b[0;36m_create_dict_ent\u001b[0;34m(self, up_ticker, statement_type, tech_type, report_name, hist_obj)\u001b[0m\n\u001b[1;32m    411\u001b[0m             \u001b[0mYAHOO_URL\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_historical_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mup_ticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_obj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 413\u001b[0;31m                 \u001b[0mcleaned_re_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_recursive_api_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_ticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    414\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    415\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/yahoofinancials/__init__.py\u001b[0m in \u001b[0;36m_recursive_api_request\u001b[0;34m(self, hist_obj, up_ticker, i)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_recursive_api_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhist_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_ticker\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0mapi_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_api_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhist_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mup_ticker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0mre_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m         \u001b[0mcleaned_re_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_clean_historical_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcleaned_re_data\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/yahoofinancials/__init__.py\u001b[0m in \u001b[0;36m_clean_api_data\u001b[0;34m(self, api_url)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[0;31m# Private Method to clean API data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_clean_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 340\u001b[0;31m         \u001b[0mraw_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    341\u001b[0m         \u001b[0mret_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    342\u001b[0m         \u001b[0mret_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'eventsData'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/yahoofinancials/__init__.py\u001b[0m in \u001b[0;36m_get_api_data\u001b[0;34m(self, api_url, tries)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_api_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mapi_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0murlopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUrlOpener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murlopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapi_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0mres_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data)\u001b[0m\n\u001b[1;32m   1754\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1755\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1756\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1757\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1758\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36mopen_https\u001b[0;34m(self, url, data)\u001b[0m\n\u001b[1;32m   1963\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mopen_https\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1964\u001b[0m             \u001b[0;34m\"\"\"Use HTTPS protocol.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1965\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_generic_http\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_https_connection\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1967\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mopen_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/urllib/request.py\u001b[0m in \u001b[0;36m_open_generic_http\u001b[0;34m(self, connection_factory, url, data)\u001b[0m\n\u001b[1;32m   1915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1916\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1917\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhttp_conn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1918\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBadStatusLine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m             \u001b[0;31m# something went wrong with the HTTP status line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mgetresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1334\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1335\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1336\u001b[0;31m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbegin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1337\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1338\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36mbegin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    304\u001b[0m         \u001b[0;31m# read until we get a non-100 response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m             \u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreason\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstatus\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mCONTINUE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/http/client.py\u001b[0m in \u001b[0;36m_read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_read_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m         \u001b[0mline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_MAXLINE\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"iso-8859-1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0m_MAXLINE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mLineTooLong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"status line\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1069\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1070\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1071\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1072\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1073\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    927\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 929\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    930\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "price_original_matrixes, adjusted_statistical_matrix = load_prices_information()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr\n",
      "custom\n"
     ]
    }
   ],
   "source": [
    "# calculate and create a huge table of results\n",
    "with open('resutls_3.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',\n",
    "                        quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    steps = 10\n",
    "    writer.writerow([\n",
    "        'per_out', \n",
    "        'det-dist-oper', \n",
    "        'avg_len',\n",
    "        *range(1, steps+1),\n",
    "        *range(1, steps+1)])\n",
    "\n",
    "    for distance_measure in ['corr', 'custom']:\n",
    "        print(distance_measure)\n",
    "        price_matrixes = calculate_price_distances(price_original_matrixes, distance_measure)\n",
    "        for operation in ['sum', 'mult']:\n",
    "            for detector in ['svm', 'lof', 'custom']:\n",
    "                for per_out in np.arange(0.05, 0.4, 0.01):\n",
    "                    step_means = []\n",
    "                    step_std = []\n",
    "                    avg_len = 0\n",
    "                    for step in range(steps):\n",
    "                        predicted_anomylies_percentage = []\n",
    "                        predicted_length = []\n",
    "                        for i in range(1, 40 - step):\n",
    "                            a, b, c = compare(adjusted_statistical_matrix[i-1], \n",
    "                                           price_matrixes[i], \n",
    "                                           adjusted_statistical_matrix[i + step], \n",
    "                                           price_matrixes[i+1 + step],\n",
    "                                           detector,\n",
    "                                           per_out,\n",
    "                                           operation)\n",
    "                            predicted_anomylies_percentage.append(a)\n",
    "                            predicted_length.append(b)\n",
    "    #                     avg_len = np.mean(predicted_length)\n",
    "                        avg_per = np.mean(predicted_anomylies_percentage)\n",
    "                        avg_len = np.mean(predicted_length)\n",
    "                        std = np.std(predicted_anomylies_percentage)\n",
    "                        step_means.append(round(avg_per, 3))\n",
    "                        step_std.append(round(std, 3))\n",
    "                    writer.writerow([\n",
    "                        round(per_out, 3), \n",
    "                        \"{}-{}-{}\".format(detector, distance_measure, operation), \n",
    "                        avg_len,\n",
    "                        *step_means,\n",
    "                        *step_std\n",
    "                    ]),\n",
    "\n",
    "# exampled list of \"true\" anomalies\n",
    "# 'AAP', 'ADS', 'AIG', 'ABC', 'COF', 'CB', 'CVS', 'DE', 'EMR', 'FITB', 'FCX', 'FTR', 'GNW', 'GILD', 'MCD', 'NTAP', 'NWL', 'PNR', 'PSA', 'PVH', 'SWN', 'TXN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABC', 'BAC', 'COF', 'CERN', 'CVS', 'DE', 'DG', 'EIX', 'EMR', 'FITB', 'F', 'FCX', 'FTR', 'GILD', 'MCD', 'MDLZ', 'MCO', 'MS', 'NWL', 'SWN', 'VRTX']\n",
      "['ALXN', 'ADS', 'AAL', 'ABC', 'BAC', 'COF', 'CVS', 'DE', 'EIX', 'EMR', 'FITB', 'FCX', 'FTR', 'GILD', 'HES', 'MAS', 'MCD', 'MDLZ', 'MCO', 'NTAP', 'NWL', 'NI', 'PSA', 'RHT', 'SWN']\n",
      "['ADS', 'AAL', 'AIG', 'ABC', 'BAC', 'COF', 'CMI', 'CVS', 'DE', 'DFS', 'EIX', 'EMR', 'FITB', 'FCX', 'FTR', 'GILD', 'INTC', 'IP', 'MCD', 'MDLZ', 'MCO', 'NTAP', 'NWL', 'NI', 'PSA', 'ROST', 'SWN', 'TGT', 'VRSN']\n",
      "['AAL', 'AIG', 'ABC', 'BLL', 'BAC', 'BBBY', 'COF', 'CVX', 'CSCO', 'CVS', 'DE', 'ECL', 'EMR', 'FITB', 'F', 'FCX', 'FTR', 'GILD', 'IP', 'MKC', 'MCD', 'MDLZ', 'MCO', 'NTAP', 'NWL', 'NI', 'NUE', 'PSA', 'SWN', 'VRSN']\n",
      "['AAP', 'ALXN', 'AAL', 'ABC', 'BLL', 'BAC', 'BBBY', 'CERN', 'CVX', 'CTXS', 'CVS', 'DE', 'EMR', 'ES', 'FITB', 'F', 'FCX', 'FTR', 'GILD', 'IP', 'KLAC', 'MAS', 'MCD', 'MDLZ', 'MCO', 'NTAP', 'NWL', 'PSA', 'SWN', 'VRSN']\n",
      "['ALXN', 'AAL', 'ABC', 'ADP', 'BLL', 'BAC', 'CTXS', 'DE', 'EMR', 'ES', 'FITB', 'F', 'FCX', 'FTR', 'GPC', 'GILD', 'KLAC', 'MCD', 'MDLZ', 'MCO', 'NTAP', 'NWL', 'NSC', 'O', 'SWN', 'VRSN']\n",
      "['ABC', 'ADP', 'BAC', 'DE', 'EMR', 'ES', 'FITB', 'FSLR', 'F', 'FCX', 'GPC', 'GILD', 'MCD', 'MDLZ', 'NTAP', 'NWL', 'NSC', 'NUE', 'OXY', 'ROK', 'SJM', 'SWN', 'VRSN']\n"
     ]
    }
   ],
   "source": [
    "# the following code in order to work with specific values/investigate companies\n",
    "\n",
    "step_means = []\n",
    "step_std = []\n",
    "avg_len = 0\n",
    "step = 0\n",
    "\n",
    "predicted_anomylies_percentage = []\n",
    "predicted_length = []\n",
    "companies = []\n",
    "# price_matrixes = calculate_price_distances(price_original_matrixes, \"custom\")\n",
    "for i in range(3, 10 - step):\n",
    "    a, b, c = compare(adjusted_statistical_matrix[i-1], \n",
    "                   price_matrixes[i], \n",
    "                   adjusted_statistical_matrix[i + step], \n",
    "                   price_matrixes[i+1 + step],\n",
    "                   detector=\"svm\",\n",
    "                   per_out=0.12,\n",
    "                   operation=\"sum\")\n",
    "    predicted_anomylies_percentage.append(a)\n",
    "    predicted_length.append(b)\n",
    "    companies.append(c)\n",
    "    print(c)\n",
    "#     print_table(\"{}-{}\".format(i, round(a, 3)), c)\n",
    "\n",
    "# avg_per = np.mean(predicted_anomylies_percentage)\n",
    "# avg_len = np.mean(predicted_length)\n",
    "# std = np.std(predicted_anomylies_percentage)\n",
    "# step_means.append(round(avg_per, 3))\n",
    "# step_std.append(round(std, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05 --- 0.04\n",
      "0.06 --- 0.05\n",
      "0.07 --- 0.07\n",
      "0.08 --- 0.08\n",
      "0.09 --- 0.09\n",
      "0.1 --- 0.1\n",
      "0.11 --- 0.1\n",
      "0.12 --- 0.11\n",
      "0.13 --- 0.12\n",
      "0.14 --- 0.13\n",
      "0.15 --- 0.14\n",
      "0.16 --- 0.15\n",
      "0.17 --- 0.15\n",
      "0.18 --- 0.15\n",
      "0.19 --- 0.17\n",
      "0.2 --- 0.18\n",
      "0.21 --- 0.19\n",
      "0.22 --- 0.2\n",
      "0.23 --- 0.2\n",
      "0.24 --- 0.21\n",
      "0.25 --- 0.23\n",
      "0.26 --- 0.24\n",
      "0.27 --- 0.23\n",
      "0.28 --- 0.24\n",
      "0.29 --- 0.25\n",
      "0.3 --- 0.26\n",
      "0.31 --- 0.26\n",
      "0.32 --- 0.27\n",
      "0.33 --- 0.27\n",
      "0.34 --- 0.29\n",
      "0.35 --- 0.3\n",
      "0.36 --- 0.3\n",
      "0.37 --- 0.3\n",
      "0.38 --- 0.32\n",
      "0.39 --- 0.32\n"
     ]
    }
   ],
   "source": [
    "for per_out in np.arange(0.05, 0.4, 0.01):\n",
    "    avg = []\n",
    "    for i in range(100):\n",
    "        k = int(len(keys_list) * per_out)\n",
    "        a = random.choices(keys_list, k=k)\n",
    "        b = random.choices(keys_list, k=k)\n",
    "\n",
    "        random_predictor = 0\n",
    "        for com_a in a:\n",
    "            if com_a in b:\n",
    "                random_predictor += 1\n",
    "\n",
    "        avg.append(random_predictor / k)\n",
    "    print(round(per_out, 2), \"---\", round(np.mean(avg),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
