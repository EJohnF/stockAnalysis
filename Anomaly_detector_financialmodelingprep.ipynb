{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlopen\n",
    "import json\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import rbf_kernel\n",
    "import time\n",
    "import random\n",
    "\n",
    "from yahoofinancials import YahooFinancials\n",
    "import math\n",
    "from sklearn.manifold import MDS\n",
    "from sklearn.neighbors import LocalOutlierFactor as LOF\n",
    "from sklearn.svm import OneClassSVM\n",
    "from scipy.stats import entropy\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import energy_distance\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import warnings\n",
    "from scipy.special import kl_div\n",
    "import csv\n",
    "from datetime import date, timedelta\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statistical metrics processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Make a function to parse JSON files from https://financialmodelingprep.com/\n",
    "2. Base on Apple ticket, retrieve the list of key statistics. The only one excluded - date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Revenue per Share',\n",
       " 'Net Income per Share',\n",
       " 'Operating Cash Flow per Share',\n",
       " 'Free Cash Flow per Share',\n",
       " 'Cash per Share',\n",
       " 'Book Value per Share',\n",
       " 'Tangible Book Value per Share',\n",
       " 'Shareholders Equity per Share',\n",
       " 'Interest Debt per Share',\n",
       " 'Market Cap',\n",
       " 'Enterprise Value',\n",
       " 'PE ratio',\n",
       " 'Price to Sales Ratio',\n",
       " 'POCF ratio',\n",
       " 'PFCF ratio',\n",
       " 'PB ratio',\n",
       " 'PTB ratio',\n",
       " 'EV to Sales',\n",
       " 'Enterprise Value over EBITDA',\n",
       " 'EV to Operating cash flow',\n",
       " 'EV to Free cash flow',\n",
       " 'Earnings Yield',\n",
       " 'Free Cash Flow Yield',\n",
       " 'Debt to Equity',\n",
       " 'Debt to Assets',\n",
       " 'Net Debt to EBITDA',\n",
       " 'Current ratio',\n",
       " 'Interest Coverage',\n",
       " 'Income Quality',\n",
       " 'Dividend Yield',\n",
       " 'Payout Ratio',\n",
       " 'SG&A to Revenue',\n",
       " 'R&D to Revenue',\n",
       " 'Intangibles to Total Assets',\n",
       " 'Capex to Operating Cash Flow',\n",
       " 'Capex to Revenue',\n",
       " 'Capex to Depreciation',\n",
       " 'Stock-based compensation to Revenue',\n",
       " 'Graham Number',\n",
       " 'Graham Net-Net',\n",
       " 'Working Capital',\n",
       " 'Tangible Asset Value',\n",
       " 'Net Current Asset Value',\n",
       " 'Invested Capital',\n",
       " 'Average Receivables',\n",
       " 'Average Payables',\n",
       " 'Average Inventory',\n",
       " 'Capex per Share']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_jsonparsed_data(url):\n",
    "    response = urlopen(url)\n",
    "    data = response.read().decode(\"utf-8\")\n",
    "    return json.loads(data)\n",
    "\n",
    "url = (\"https://financialmodelingprep.com/api/v3/company-key-metrics/AAPL?period=quarter\")\n",
    "apple = get_jsonparsed_data(url)\n",
    "\n",
    "keys = []\n",
    "for key in apple['metrics'][0]:\n",
    "    if not key == 'date':\n",
    "        keys.append(key)\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SP500_symbols = ['AAPL', 'ABT', 'ABBV', 'ACN', 'ACE', 'ADBE', 'ADT', 'AAP', 'AES', 'AET', 'AFL', 'AMG', 'A', 'GAS', 'ARE', 'APD', 'AKAM', 'AA', 'AGN', 'ALXN', 'ALLE', 'ADS', 'ALTR', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'AON', 'APA', 'AIV', 'AMAT', 'ADM', 'AIZ', 'T', 'ADSK', 'ADP', 'AN', 'AZO', 'AVB', 'AVY', 'BHI', 'BLL', 'BAC', 'BK', 'BCR', 'BXLT', 'BAX', 'BBT', 'BDX', 'BBBY', 'BBY', 'BLX', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'BRCM', 'CHRW', 'CA', 'CVC', 'COG', 'CAM', 'CPB', 'COF', 'CAH', 'HSIC', 'KMX', 'CCL', 'CAT', 'CBG', 'CBS', 'CELG', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHK', 'CVX', 'CMG', 'CB', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CTXS', 'CLX', 'CME', 'CMS', 'COH', 'KO', 'CCE', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CSC', 'CAG', 'COP', 'CNX', 'ED', 'STZ', 'GLW', 'COST', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DLPH', 'DAL', 'XRAY', 'DVN', 'DO', 'DTV', 'DFS', 'DISCA', 'DISCK', 'DLTR', 'D', 'DOV', 'DOW', 'DPS', 'DTE', 'DD', 'DUK', 'DNB', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMC', 'EMR', 'ENDP', 'ESV', 'ETR', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'EXC', 'EXPE', 'EXPD', 'ESRX', 'XOM', 'FFIV', 'FB', 'FAST', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC', 'FTI', 'F', 'FOSL', 'BEN', 'FCX', 'FTR', 'GME', 'GPS', 'GRMN', 'GD', 'GE', 'GGP', 'GIS', 'GM', 'GPC', 'GNW', 'GILD', 'GS', 'GT', 'GOOGL', 'GOOG', 'GWW', 'HAL', 'HBI', 'HOG', 'HAR', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HCN', 'HP', 'HES', 'HPQ', 'HD', 'HON', 'HRL', 'HSP', 'HST', 'HCBK', 'HUM', 'HBAN', 'ITW', 'IR', 'INTC', 'ICE', 'IBM', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IRM', 'JEC', 'JBHT', 'JNJ', 'JCI', 'JOY', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'GMCR', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KRFT', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LM', 'LEG', 'LEN', 'LVLT', 'LUK', 'LLY', 'LNC', 'LLTC', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MNK', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MJN', 'MMV', 'MDT', 'MRK', 'MET', 'KORS', 'MCHP', 'MU', 'MSFT', 'MHK', 'TAP', 'MDLZ', 'MON', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MUR', 'MYL', 'NDAQ', 'NOV', 'NAVI', 'NTAP', 'NFLX', 'NWL', 'NFX', 'NEM', 'NWSA', 'NEE', 'NLSN', 'NKE', 'NI', 'NE', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'OI', 'PCAR', 'PLL', 'PH', 'PDCO', 'PAYX', 'PNR', 'PBCT', 'POM', 'PEP', 'PKI', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PBI', 'PCL', 'PNC', 'RL', 'PPG', 'PPL', 'PX', 'PCP', 'PCLN', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'QRVO', 'PWR', 'QCOM', 'DGX', 'RRC', 'RTN', 'O', 'REGN', 'RF', 'RSG', 'RAI', 'RHI', 'ROK', 'COL', 'ROP', 'ROST', 'RLD', 'R', 'CRM', 'SNDK', 'SCG', 'SLB', 'SNI', 'STX', 'SEE', 'SRE', 'SHW', 'SPG', 'SWKS', 'SLG', 'SJM', 'SNA', 'SO', 'LUV', 'SWN', 'SE', 'STJ', 'SWK', 'SPLS', 'SBUX', 'HOT', 'STT', 'SRCL', 'SYK', 'STI', 'SYMC', 'SYY', 'TROW', 'TGT', 'TEL', 'TE', 'TGNA', 'THC', 'TDC', 'TSO', 'TXN', 'TXT', 'HSY', 'TRV', 'TMO', 'TIF', 'TWX', 'TWC', 'TJX', 'TMK', 'TSCO', 'RIG', 'TRIP', 'FOXA', 'TSN', 'TYC', 'UA', 'UNP', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'URBN', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'DIS', 'WM', 'WAT', 'ANTM', 'WFC', 'WDC', 'WU', 'WY', 'WHR', 'WFM', 'WMB', 'WEC', 'WYN', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XL', 'XYL', 'YHOO', 'YUM', 'ZION', 'ZTS']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve the statistical data for all SP500 companies \n",
    "data = []\n",
    "for symbol in SP500_symbols:\n",
    "    url = \"https://financialmodelingprep.com/api/v3/company-key-metrics/{}?period=quarter\".format(symbol)\n",
    "    stock = get_jsonparsed_data(url)\n",
    "    data.append(stock)\n",
    "#     try:\n",
    "#         print(symbol, len(stock['metrics']))\n",
    "#     except:\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only those companies which has metrics\n",
    "cleaned_data = []\n",
    "for symbol in data:\n",
    "    try:\n",
    "        symbol['metrics']\n",
    "        cleaned_data.append(symbol)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save only those companies which has all 41 metric\n",
    "full_info_data = []\n",
    "for symbol in cleaned_data:\n",
    "    try:\n",
    "        if len(symbol['metrics']) > 30:\n",
    "            full_info_data.append(symbol)\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert date from dictionary key-value representation, to the matrix of features\n",
    "resulted `database` variable is 3-D. \n",
    "1. First dimention - the list of periods (quarterly)\n",
    "2. Second dimention - the list of companies\n",
    "3. Third - list of features described given company in a given period (taken from key metrics)\n",
    "\n",
    "So, for example [0][2] - is an array of features for the company with index 2 in the first quartal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_databases = []\n",
    "for i in range(42):\n",
    "    companies = []\n",
    "    for symbol in full_info_data:\n",
    "        company = []\n",
    "        for key in keys:\n",
    "            try:\n",
    "                company.append(float(symbol['metrics'][i][key]))\n",
    "            except:\n",
    "                company.append(0)\n",
    "        companies.append(company)\n",
    "    state_databases.append(companies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in each quartal we need to define the kernal = distance matrix between companies. \n",
    "# For that firstly normalize the data, and then calculate rbf kernel\n",
    "def kernalise_database(databases, norm=\"max\", gamma=1):\n",
    "    kernalizes_databases = []\n",
    "    for base in databases:\n",
    "        normal = normalize(np.array(base), norm=\"max\")\n",
    "        kernalizes_databases.append(rbf_kernel(normal, normal, gamma=gamma))\n",
    "    return kernalizes_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "414\n"
     ]
    }
   ],
   "source": [
    "# the list of all symbols which are in the database (~has full set of statistics in a given period of time)\n",
    "keys_list = []\n",
    "for symbol in full_info_data:\n",
    "    keys_list.append(symbol['symbol'])\n",
    "print(len(keys_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prices processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_distance_matrix(historical_prices, approach='corr'):\n",
    "    prices_matrix = historical_prices\n",
    "\n",
    "# now prices_matrix - contains list of price changes for each company in a given period\n",
    "# the next step - create a kernel (distance matrix) For that there are three possible approaches\n",
    "\n",
    "# defaul one - correlation\n",
    "    if approach=='corr':\n",
    "        return np.absolute(np.corrcoef(prices_matrix))\n",
    "    \n",
    "#     entropy between two series\n",
    "    if approach=='KL':\n",
    "        distances_matrix = np.zeros((len(prices_matrix), len(prices_matrix)))\n",
    "        for i, stock in enumerate(prices_matrix):\n",
    "            for j, stock_2 in enumerate(prices_matrix):\n",
    "                distances_matrix[i][j] = entropy(stock, qk=stock_2)\n",
    "        return distances_matrix\n",
    "    \n",
    "#     custom - two companies closer when they have more corelated changed (the same direction of change in a day)\n",
    "    if approach=='custom':\n",
    "        distances_matrix = np.zeros((len(prices_matrix), len(prices_matrix)))\n",
    "        for i, stock in enumerate(prices_matrix):\n",
    "            for j, stock_2 in enumerate(prices_matrix):\n",
    "                k = 0\n",
    "                for q, price in enumerate(stock):\n",
    "                    try:\n",
    "                        if (price > 0 and stock_2[q] > 0) or (price <= 0 and stock_2[q] <= 0):\n",
    "                            k += 1\n",
    "                    except:\n",
    "                        pass\n",
    "                distances_matrix[i][j] = abs(k/len(stock) - 0.5) * 2 if len(stock) > 0 else 1\n",
    "        return distances_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prices(keys, start_date, end_date):\n",
    "    price_matrix = []\n",
    "    dates_matrix = []\n",
    "    for key in keys:\n",
    "        url = \"https://financialmodelingprep.com/api/v3/historical-price-full/{}?from={}&to={}\".format(key, start_date, end_date)\n",
    "        data = get_jsonparsed_data(url)\n",
    "        try:\n",
    "            price_matrix.append(list(map(lambda x: x['changePercent'], data['historical'])))\n",
    "            dates_matrix.append(list(map(lambda x: datetime.datetime.strptime(x['date'], \"%Y-%m-%d\").date(), data['historical'])))\n",
    "        except:\n",
    "            print(key, data)\n",
    "    return price_matrix, dates_matrix[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bounary_indexes(start_date, end_date, dates):\n",
    "    index_a = 0\n",
    "    index_b = 0\n",
    "    j = 0\n",
    "    while dates[j] < start_date:\n",
    "        j+=1\n",
    "    index_a = j\n",
    "\n",
    "    while dates[j] < end_date:\n",
    "        j+=1\n",
    "    index_b = j + 1\n",
    "    \n",
    "    return index_a, index_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_prices_list - 2D array, 1st dimenstion - companies, 2nd - prices from initial date to final\n",
    "# dates - the list of stock exchange working dates\n",
    "def load_prices_information(full_prices_list, dates):\n",
    "    # original price matrixes (without normalisation and calculating the distance)\n",
    "    price_original_matrixes = []\n",
    "    # number of calendar days in each quartal\n",
    "    intervals = [91, 91, 89, 90]\n",
    "    # start date, and then with each step move it on corresponding number of days\n",
    "    current_date = date(2009, 7, 1)\n",
    "    # number of quartals\n",
    "    number_of_intervals = 41\n",
    "    \n",
    "    index = 0\n",
    "    for i in range(number_of_intervals):\n",
    "    #   calculate start and end date for given â„– quartal\n",
    "        start_date = \"\"\n",
    "        end_date = \"\"\n",
    "        increased_days = 0\n",
    "    #   for leap year, there is one additional day in the first quartal\n",
    "        if i == 10 or i == 26:\n",
    "            increased_days = 1\n",
    "        start_date = current_date\n",
    "        end_date = (current_date + timedelta(days=intervals[i%4] + increased_days))\n",
    "        current_date = current_date + timedelta(days=intervals[i%4] + 1 + increased_days)\n",
    "\n",
    "        current_quartal = []\n",
    "        \n",
    "        low, high = find_bounary_indexes(start_date, end_date, dates)\n",
    "        for company in full_prices_list:\n",
    "            current_quartal.append(company[low: high])\n",
    "            \n",
    "        price_original_matrixes.append(current_quartal)\n",
    "\n",
    "    return price_original_matrixes\n",
    "    \n",
    "    \n",
    "# form distance matrixes from prices. Approach = 'corr'/'KL'/'custom'\n",
    "# original matrixes - 3D quartal-company-prices\n",
    "def calculate_price_distances(original_matrixes, approach='corr'):\n",
    "        # matrixes of distances between companies based on price and default approach (correlation)\n",
    "    price_distance_matrixes = []\n",
    "    for i, matrix in enumerate(original_matrixes):\n",
    "        distance_matrix = create_distance_matrix(historical_prices=matrix, approach=approach)\n",
    "        price_distance_matrixes.append(distance_matrix)\n",
    "    return price_distance_matrixes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load raw prices matrixes\n",
    "full_prices_list, dates = get_prices(keys_list, \"2009-7-1\", \"2019-11-1\")\n",
    "price_original_matrixes = load_prices_information(full_prices_list, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['AAPL', 'ABT', 'ABBV', 'ACN', 'ADBE', 'AAP', 'AES', 'AFL', 'AMG', 'A', 'ARE', 'APD', 'AKAM', 'AGN', 'ALXN', 'ADS', 'MO', 'AMZN', 'AEE', 'AAL', 'AEP', 'AXP', 'AIG', 'AMT', 'AMP', 'ABC', 'AME', 'AMGN', 'APH', 'ADI', 'AON', 'APA', 'AIV', 'AMAT', 'ADM', 'AIZ', 'T', 'ADSK', 'ADP', 'AN', 'AZO', 'AVB', 'AVY', 'BLL', 'BAC', 'BK', 'BAX', 'BBT', 'BDX', 'BBBY', 'BBY', 'HRB', 'BA', 'BWA', 'BXP', 'BSX', 'BMY', 'CHRW', 'COG', 'CPB', 'COF', 'CAH', 'HSIC', 'KMX', 'CCL', 'CAT', 'CBS', 'CELG', 'CNP', 'CTL', 'CERN', 'CF', 'SCHW', 'CHK', 'CVX', 'CMG', 'CB', 'CI', 'XEC', 'CINF', 'CTAS', 'CSCO', 'C', 'CTXS', 'CLX', 'CME', 'CMS', 'KO', 'CTSH', 'CL', 'CMCSA', 'CMA', 'CAG', 'COP', 'CNX', 'ED', 'STZ', 'GLW', 'COST', 'CCI', 'CSX', 'CMI', 'CVS', 'DHI', 'DHR', 'DRI', 'DVA', 'DE', 'DAL', 'XRAY', 'DVN', 'DO', 'DFS', 'DISCA', 'DLTR', 'D', 'DOV', 'DPS', 'DTE', 'DD', 'DUK', 'ETFC', 'EMN', 'ETN', 'EBAY', 'ECL', 'EIX', 'EW', 'EA', 'EMR', 'ENDP', 'ESV', 'ETR', 'EOG', 'EQT', 'EFX', 'EQIX', 'EQR', 'ESS', 'EL', 'ES', 'EXC', 'EXPE', 'EXPD', 'XOM', 'FFIV', 'FB', 'FAST', 'FDX', 'FIS', 'FITB', 'FSLR', 'FE', 'FISV', 'FLIR', 'FLS', 'FLR', 'FMC', 'F', 'FOSL', 'BEN', 'FCX', 'FTR', 'GME', 'GPS', 'GRMN', 'GD', 'GE', 'GIS', 'GM', 'GPC', 'GNW', 'GILD', 'GS', 'GT', 'GOOGL', 'GWW', 'HAL', 'HBI', 'HOG', 'HRS', 'HIG', 'HAS', 'HCA', 'HCP', 'HP', 'HES', 'HPQ', 'HD', 'HON', 'HRL', 'HST', 'HUM', 'HBAN', 'ITW', 'IR', 'INTC', 'ICE', 'IBM', 'IP', 'IPG', 'IFF', 'INTU', 'ISRG', 'IVZ', 'IRM', 'JEC', 'JBHT', 'JNJ', 'JCI', 'JPM', 'JNPR', 'KSU', 'K', 'KEY', 'KMB', 'KIM', 'KMI', 'KLAC', 'KSS', 'KR', 'LB', 'LLL', 'LH', 'LRCX', 'LM', 'LEG', 'LEN', 'LLY', 'LNC', 'LMT', 'L', 'LOW', 'LYB', 'MTB', 'MAC', 'M', 'MNK', 'MRO', 'MPC', 'MAR', 'MMC', 'MLM', 'MAS', 'MA', 'MAT', 'MKC', 'MCD', 'MCK', 'MDT', 'MRK', 'MET', 'MCHP', 'MU', 'MSFT', 'MHK', 'TAP', 'MDLZ', 'MNST', 'MCO', 'MS', 'MOS', 'MSI', 'MUR', 'MYL', 'NDAQ', 'NOV', 'NTAP', 'NFLX', 'NWL', 'NEM', 'NWSA', 'NEE', 'NLSN', 'NKE', 'NI', 'NE', 'NBL', 'JWN', 'NSC', 'NTRS', 'NOC', 'NRG', 'NUE', 'NVDA', 'ORLY', 'OXY', 'OMC', 'OKE', 'ORCL', 'OI', 'PCAR', 'PH', 'PDCO', 'PAYX', 'PNR', 'PBCT', 'PEP', 'PKI', 'PRGO', 'PFE', 'PCG', 'PM', 'PSX', 'PNW', 'PXD', 'PBI', 'PNC', 'RL', 'PPG', 'PPL', 'PFG', 'PG', 'PGR', 'PLD', 'PRU', 'PEG', 'PSA', 'PHM', 'PVH', 'PWR', 'QCOM', 'DGX', 'RRC', 'RTN', 'O', 'REGN', 'RF', 'RSG', 'RHI', 'ROK', 'ROP', 'ROST', 'R', 'CRM', 'SLB', 'STX', 'SEE', 'SRE', 'SHW', 'SPG', 'SWKS', 'SLG', 'SJM', 'SNA', 'SO', 'LUV', 'SWN', 'SWK', 'SBUX', 'STT', 'SRCL', 'SYK', 'STI', 'SYMC', 'SYY', 'TROW', 'TGT', 'TEL', 'TGNA', 'THC', 'TDC', 'TXN', 'TXT', 'HSY', 'TRV', 'TMO', 'TIF', 'TJX', 'TMK', 'TSCO', 'RIG', 'TRIP', 'TSN', 'UNP', 'UNH', 'UPS', 'URI', 'UTX', 'UHS', 'UNM', 'URBN', 'VFC', 'VLO', 'VAR', 'VTR', 'VRSN', 'VZ', 'VRTX', 'VIAB', 'V', 'VNO', 'VMC', 'WMT', 'WBA', 'DIS', 'WM', 'WAT', 'ANTM', 'WFC', 'WDC', 'WU', 'WY', 'WHR', 'WMB', 'WEC', 'WYNN', 'XEL', 'XRX', 'XLNX', 'XYL', 'YUM', 'ZION', 'ZTS']\n"
     ]
    }
   ],
   "source": [
    "print(keys_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# matrix: 2D array of distance measure of variables\n",
    "# trevial approach based on mean distance \n",
    "# return the list of indexes with detected anomalies\n",
    "def find_anomaly_custom(matrix, per_out, cloud_number=10):\n",
    "    result = []\n",
    "    mean_distance = np.mean(matrix) / 3\n",
    "    # if cloud_number of nearest points located father than mean_distance, then it's anomaly  \n",
    "    avg_distances = []\n",
    "    for i, stock in enumerate(matrix):\n",
    "        A = np.array(stock)\n",
    "        idx = np.argpartition(A, cloud_number)\n",
    "        avg_distances.append(np.mean(A[idx[:cloud_number]]))\n",
    "        \n",
    "    number_anomaly = int(per_out * len(matrix[0]))\n",
    "    return np.argpartition(np.array(avg_distances), number_anomaly)[:number_anomaly]\n",
    "\n",
    "# base on local outlier factor\n",
    "def find_anomaly_lof(matrix, per_out):\n",
    "    detector = LOF(metric='precomputed', contamination=per_out)\n",
    "    inlines = detector.fit_predict(matrix)\n",
    "    result = []\n",
    "    for i, res in enumerate(inlines):\n",
    "        if res == -1:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "# based on SVM\n",
    "def find_anomaly_svm(matrix, per_out):\n",
    "    detector = OneClassSVM(kernel='precomputed', nu=per_out)\n",
    "    inlines = detector.fit_predict(matrix)\n",
    "    result = []\n",
    "    for i, res in enumerate(inlines):\n",
    "        if res == -1:\n",
    "            result.append(i)\n",
    "    return result\n",
    "\n",
    "# use pre-computed matrix of distances!!\n",
    "# general function to find outliers, approach one of ['custom', 'lof', 'svm']\n",
    "def find_anomaly(matrix, approach, per_out, additional=10):\n",
    "    if approach=='custom':\n",
    "        return find_anomaly_custom(matrix, per_out, additional)\n",
    "    if approach=='lof':\n",
    "        return find_anomaly_lof(matrix, per_out)\n",
    "    if approach=='svm':\n",
    "        return find_anomaly_svm(matrix, per_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two distance(kernel) matrixes. Statistical and price. \n",
    "\n",
    "stat_1, price_1 - training period.\n",
    "stat_2, price_2 - test period\n",
    "\n",
    "1. Apply sum or multiply operator for kernels. \n",
    "2. Based on new kernel, apply anomaly detector approach.\n",
    "3. Compare two arrays of anomalieys. \n",
    "4. Return three values: the proporation of anomalies from the first (training) set which are also exist in the second (test) set, the number of detected anomalies in the first set and the list of symbols of intersections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    lst3 = [value for value in lst1 if value in lst2] \n",
    "    return lst3 \n",
    "\n",
    "def get_anomalies_list(stat, price, detector, per_out, operation='sum', cloud_number=10):\n",
    "    K = (stat + price) / 2\n",
    "    if operation == 'mult':\n",
    "        K = stat * price\n",
    "    return find_anomaly(K, detector, per_out, cloud_number)\n",
    "\n",
    "def indexes_to_symbol(indexes):\n",
    "    symbols = []\n",
    "    for i in indexes:\n",
    "        symbols.append(keys_list[i])\n",
    "    return symbols\n",
    "    \n",
    "def compare(stat_1, price_1, stat_2, price_2, detector, per_out, operation='sum', cloud_number=10):\n",
    "    train_anomaly = get_anomalies_list(stat_1, price_1, detector, per_out, operation, cloud_number)\n",
    "    test_anomaly = get_anomalies_list(stat_2, price_2, detector, per_out, operation, cloud_number)\n",
    "    common = intersection(train_anomaly, test_anomaly)\n",
    "    return len(common)/len(train_anomaly), len(train_anomaly), indexes_to_symbol(common)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41, 414)\n"
     ]
    }
   ],
   "source": [
    "print(np.shape(price_original_matrixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and create a huge table of results\n",
    "with open('resutls_8.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile, delimiter=',',\n",
    "                        quotechar='|', quoting=csv.QUOTE_MINIMAL)\n",
    "    steps = 10 #number of quarters to which we compare\n",
    "    writer.writerow([\n",
    "        'per_out', \n",
    "        'det-dist-oper', \n",
    "        'gamma',\n",
    "        'cloud_number',\n",
    "        'avg_len',\n",
    "        *range(1, steps+1),\n",
    "        *range(1, steps+1)])\n",
    "\n",
    "#     'corr'/'custom'\n",
    "    for distance_measure in ['custom']:\n",
    "        price_matrixes = calculate_price_distances(price_original_matrixes, distance_measure)\n",
    "#         'sum'/'mult'\n",
    "        operation = 'sum'\n",
    "        for gamma in np.arange(0.4, 0.6, 0.05):\n",
    "            stat_matrixes = kernalise_database(state_databases, gamma=gamma)\n",
    "#             'svm'/'lof'/'custom'\n",
    "            for detector in ['custom']:\n",
    "                for cloud_number in np.arange(10, 20, 5):\n",
    "                    for per_out in np.arange(0.10, 0.13, 0.02):\n",
    "                        step_means = []\n",
    "                        step_std = []\n",
    "                        avg_len = 0\n",
    "                        for step in range(steps):\n",
    "                            predicted_anomylies_percentage = []\n",
    "                            predicted_length = []\n",
    "                            for i in range(1, 40 - step):\n",
    "                                a, b, c = compare(stat_matrixes[i-1], \n",
    "                                               price_matrixes[i], \n",
    "                                               stat_matrixes[i + step], \n",
    "                                               price_matrixes[i+1 + step],\n",
    "                                               detector,\n",
    "                                               per_out,\n",
    "                                               operation,\n",
    "                                                cloud_number)\n",
    "                                predicted_anomylies_percentage.append(a)\n",
    "                                predicted_length.append(b)\n",
    "        #                     avg_len = np.mean(predicted_length)\n",
    "                            avg_per = np.mean(predicted_anomylies_percentage)\n",
    "                            avg_len = np.mean(predicted_length)\n",
    "                            std = np.std(predicted_anomylies_percentage)\n",
    "                            step_means.append(round(avg_per, 3))\n",
    "                            step_std.append(round(std, 3))\n",
    "                        writer.writerow([\n",
    "                            round(per_out, 3), \n",
    "                            \"{}-{}-{}\".format(detector, distance_measure, operation),\n",
    "                            round(gamma,2),\n",
    "                            cloud_number,\n",
    "                            round(avg_len,1),\n",
    "                            *step_means,\n",
    "                            *step_std\n",
    "                        ]),\n",
    "\n",
    "# exampled list of \"true\" anomalies\n",
    "# 'AAP', 'ADS', 'AIG', 'ABC', 'COF', 'CB', 'CVS', 'DE', 'EMR', 'FITB', 'FCX', 'FTR', 'GNW', 'GILD', 'MCD', 'NTAP', 'NWL', 'PNR', 'PSA', 'PVH', 'SWN', 'TXN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
